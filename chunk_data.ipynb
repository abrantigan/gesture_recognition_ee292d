{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a list of data file paths, return one list containing recording data, each element in list \n",
    "# is an np.array containing entire recording\n",
    "def csv_to_np(data_list_name):\n",
    "    recordings = []\n",
    "    for ind, recording in enumerate(data_list_name):\n",
    "        recording_np = np.genfromtxt(recording, delimiter=',')\n",
    "        recordings.append(recording_np)\n",
    "    return recordings\n",
    "\n",
    "\n",
    "# Transfer from 10s recordings to 300, 3 data chunks\n",
    "# Each recording contributes NUM_CHUNKS_PER_RECORDING chunks to output to retain evenly distributed data\n",
    "# ASSUMPTIONS: input parameter recordings is a list of np arrays of recordings\n",
    "def chunk_data(recordings, n_samples_per_chunk, n_meas_per_samp, n_chunks_per_rec):\n",
    "    NUM_SAMPLES_PER_CHUNK = n_samples_per_chunk\n",
    "    NUM_MEASUREMENTS_PER_SAMPLE = n_meas_per_samp\n",
    "    NUM_CHUNKS_PER_RECORDING = n_chunks_per_rec \n",
    "    \n",
    "    out = []\n",
    "    for recording in recordings:\n",
    "        press_inds = np.argwhere(recording[:,0]==1).squeeze()\n",
    "        num_samples_pressed = recording[press_inds,:].shape[0]\n",
    "        \n",
    "        # CASE 0: check that we have at least 300 samples in entire recording\n",
    "        if recording.shape[0] < NUM_SAMPLES_PER_CHUNK:\n",
    "            sample_pad = np.zeros((NUM_SAMPLES_PER_CHUNK,NUM_MEASUREMENTS_PER_SAMPLE))\n",
    "            sample_pad[0:recording.shape[0],:] = recording[:,1:]\n",
    "            out.append(sample_pad)\n",
    "        \n",
    "        # CASE 1: we have enough  samples to fill out chunks with only press data\n",
    "        elif num_samples_pressed >= NUM_SAMPLES_PER_CHUNK + NUM_CHUNKS_PER_RECORDING:\n",
    "            \n",
    "            # First chunk (always capture very beginning of data)\n",
    "            first_chunk_start = press_inds[0]\n",
    "            out.append(recording[first_chunk_start:first_chunk_start+NUM_SAMPLES_PER_CHUNK,1:])\n",
    "            \n",
    "            # Last chunk (always capture very end of data)\n",
    "            last_chunk_start = press_inds[-1]-NUM_SAMPLES_PER_CHUNK+1\n",
    "            out.append(recording[last_chunk_start:last_chunk_start+NUM_SAMPLES_PER_CHUNK,1:])\n",
    "            \n",
    "            # Middle chunks (evenly space from first chunk for simplicity)\n",
    "            step = num_samples_pressed // NUM_CHUNKS_PER_RECORDING\n",
    "            prev_chunk_start = first_chunk_start\n",
    "            for i in range(NUM_CHUNKS_PER_RECORDING-2):\n",
    "                next_chunk_start = prev_chunk_start+step\n",
    "                out.append(recording[next_chunk_start:next_chunk_start+NUM_SAMPLES_PER_CHUNK,1:])\n",
    "                prev_chunk_start = next_chunk_start\n",
    "                \n",
    "        # CASE 2: We don't have enough samples to fill chunks: need to pull from surrounding data\n",
    "        elif num_samples_pressed < NUM_SAMPLES_PER_CHUNK + NUM_CHUNKS_PER_RECORDING:\n",
    "            # First chunk: captures until end of recording, starting however many samples before needed to do that\n",
    "            first_chunk_start = max(0, press_inds[-1]-NUM_SAMPLES_PER_CHUNK+1)\n",
    "            out.append(recording[first_chunk_start:first_chunk_start+NUM_SAMPLES_PER_CHUNK,1:])\n",
    "            \n",
    "            # Last chunk: captures from start of recording, going however many samples after needed to do that\n",
    "            last_chunk_end = min(recording.shape[0], press_inds[0]+NUM_SAMPLES_PER_CHUNK)\n",
    "            out.append(recording[last_chunk_end-NUM_SAMPLES_PER_CHUNK:last_chunk_end,1:])\n",
    "            \n",
    "            # Middle chunks: (evenly space from first chunk for simplicity)\n",
    "            step = (last_chunk_end - first_chunk_start+1) // NUM_CHUNKS_PER_RECORDING\n",
    "            prev_chunk_start = first_chunk_start\n",
    "            for i in range(NUM_CHUNKS_PER_RECORDING-2):\n",
    "                next_chunk_start = prev_chunk_start+step\n",
    "                out.append(recording[next_chunk_start:next_chunk_start+NUM_SAMPLES_PER_CHUNK,1:])\n",
    "                prev_chunk_start = next_chunk_start\n",
    "                \n",
    "    return out\n",
    "\n",
    "## for every name and every letter\n",
    "## output_A_name\n",
    "\n",
    "# # Note: train obtained from running Splitting_up_data.ipynb on raw training data\n",
    "# train = ['Data/A_lauren_11.csv', 'Data/A_lauren_18.csv', 'Data/A_lauren_20.csv', 'Data/B_lauren_14.csv',\\\n",
    "#          'Data/B_lauren_15.csv', 'Data/B_lauren_17.csv', 'Data/C_lauren_28.csv', 'Data/C_lauren_18.csv', \\\n",
    "#          'Data/C_lauren_3.csv', 'Data/D_lauren_8.csv', 'Data/D_lauren_23.csv', 'Data/D_lauren_4.csv',\\\n",
    "#          'Data/E_lauren_5.csv', 'Data/E_lauren_7.csv', 'Data/E_lauren_25.csv', 'Data/F_lauren_4.csv', \\\n",
    "#          'Data/F_lauren_26.csv', 'Data/F_lauren_2.csv', 'Data/G_lauren_4.csv', 'Data/G_lauren_8.csv',\\\n",
    "#          'Data/G_lauren_13.csv', 'Data/H_lauren_29.csv', 'Data/H_lauren_23.csv', 'Data/H_lauren_22.csv', \\\n",
    "#          'Data/I_lauren_19.csv', 'Data/I_lauren_22.csv', 'Data/I_lauren_8.csv', 'Data/J_lauren_1.csv', \\\n",
    "#          'Data/J_lauren_16.csv', 'Data/J_lauren_23.csv', 'Data/K_lauren_1.csv', 'Data/K_lauren_10.csv', \\\n",
    "#          'Data/K_lauren_23.csv', 'Data/L_lauren_30.csv', 'Data/L_lauren_6.csv', 'Data/L_lauren_26.csv', \\\n",
    "#          'Data/M_lauren_21.csv', 'Data/M_lauren_3.csv', 'Data/M_lauren_19.csv', 'Data/N_lauren_1.csv', \\\n",
    "#          'Data/N_lauren_14.csv', 'Data/N_lauren_25.csv', 'Data/O_lauren_17.csv', 'Data/O_lauren_10.csv', \\\n",
    "#          'Data/O_lauren_28.csv', 'Data/P_lauren_22.csv', 'Data/P_lauren_29.csv', 'Data/P_lauren_3.csv', \\\n",
    "#          'Data/Q_lauren_10.csv', 'Data/Q_lauren_29.csv', 'Data/Q_lauren_23.csv', 'Data/R_lauren_4.csv', \\\n",
    "#          'Data/R_lauren_8.csv', 'Data/R_lauren_12.csv', 'Data/S_lauren_27.csv', 'Data/S_lauren_12.csv', \\\n",
    "#          'Data/S_lauren_18.csv', 'Data/T_lauren_9.csv', 'Data/T_lauren_21.csv', 'Data/T_lauren_7.csv', \\\n",
    "#          'Data/U_lauren_11.csv', 'Data/U_lauren_24.csv', 'Data/U_lauren_3.csv', 'Data/V_lauren_5.csv', \\\n",
    "#          'Data/V_lauren_26.csv', 'Data/V_lauren_10.csv', 'Data/W_lauren_19.csv', 'Data/W_lauren_2.csv', \\\n",
    "#          'Data/W_lauren_18.csv', 'Data/X_lauren_2.csv', 'Data/X_lauren_28.csv', 'Data/X_lauren_8.csv', \\\n",
    "#          'Data/Y_lauren_29.csv', 'Data/Y_lauren_3.csv', 'Data/Y_lauren_17.csv', 'Data/Z_lauren_8.csv', \\\n",
    "#          'Data/Z_lauren_20.csv', 'Data/Z_lauren_23.csv']\n",
    "# recordings = csv_to_np(train)\n",
    "# assert len(recordings) == len(train)\n",
    "# NUM_SAMPLES_PER_CHUNK = 300\n",
    "# NUM_MEASUREMENTS_PER_SAMPLE = 3\n",
    "# NUM_CHUNKS_PER_RECORDING = 5\n",
    "# chunks = chunk_data(recordings, NUM_SAMPLES_PER_CHUNK, NUM_MEASUREMENTS_PER_SAMPLE, NUM_CHUNKS_PER_RECORDING)\n",
    "# print(\"executed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_multiple_lines(file_name, lines_to_append):\n",
    "    # Open the file in append & read mode ('a+')\n",
    "    with open(file_name, \"a+\") as file_object:\n",
    "        appendEOL = False\n",
    "        # Move read cursor to the start of file.\n",
    "        file_object.seek(0)\n",
    "        # Check if file is not empty\n",
    "        data = file_object.read(100)\n",
    "        if len(data) > 0:\n",
    "            appendEOL = True\n",
    "        # Iterate over each string in the list\n",
    "        for line in lines_to_append:\n",
    "            # If file is not empty then append '\\n' before first line for\n",
    "            # other lines always append '\\n' before appending line\n",
    "            if appendEOL == True:\n",
    "                file_object.write(\"\\n\")\n",
    "                \n",
    "            else:\n",
    "                appendEOL = True\n",
    "            # Append element at the end of file\n",
    "#             file_object.write(line)\n",
    "                np.savetxt(file_name,line,delimiter=',',fmt='%d')     #numpy savetxt\n",
    "    #     file_object.close()\n",
    "        \n",
    "#given list of arrays, write to text folder/file.txt\n",
    "def np_to_txt(chunks, fileName):\n",
    "    with open(fileName, \"a+\") as f:\n",
    "        for i in range(len(chunks)):\n",
    "            for j in range(len(chunks[i])):\n",
    "                f.write('\\n\\n-,-,-\\n')\n",
    "                for k in range(len(chunks[i][j])):\n",
    "                    np.savetxt(f, [chunks[i][j][k]], fmt=\"%3.3f\", delimiter=',', newline='')\n",
    "                    f.write('\\n')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing: K_lauren.txt\n",
      "Writing: K_katherine.txt\n",
      "Writing: K_annie.txt\n"
     ]
    }
   ],
   "source": [
    "############################################################################################################################\n",
    "NUM_SAMPLES_PER_CHUNK = 300\n",
    "NUM_MEASUREMENTS_PER_SAMPLE = 3\n",
    "NUM_CHUNKS_PER_RECORDING = 5   \n",
    "\n",
    "path = r\"D:\\292d\\letters\"#r\"C:\\Users\\kathe\\Documents\\GitHub\\tensorflow\\tensorflow\\lite\\micro\\examples\\magic_wand\\train\\letters\"\n",
    "write_path = r\"D:\\292d\\data\"#r\"C:\\Users\\kathe\\Documents\\GitHub\\tensorflow\\tensorflow\\lite\\micro\\examples\\magic_wand\\train\\data\"\n",
    "names=[\"lauren\",\"katherine\",\"annie\"]#,\"hallie\"]\n",
    "letters=[\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\",\n",
    "          \"apostrophe\",\"backspace\",\"comma\",\"done\",\"exclamation_point\", \"period\",\"question_mark\",\"quotes\",\"slash\",\"space\",\"negative\"]\n",
    "for l in letters: \n",
    "    for n in names:\n",
    "        letter_list =[]\n",
    "        fileWrite =write_path+'\\\\'+l+'\\output_'+l+'_'+n+'.txt' ##CHANGE TO OUTPUT \n",
    "        open(fileWrite, 'w').close()\n",
    "        print('Writing: '+l+'_'+n+'.txt')       \n",
    "        for file in glob.glob(path+'\\\\'+l+'\\\\'+l+'_'+n +'_'+'*.csv'):\n",
    "            ##for all lauren's As, write -,-,-  write list of arrays to .txt file (output_A_lauren)\n",
    "            recordings = csv_to_np([file])\n",
    "\n",
    "            chunks = chunk_data(recordings, NUM_SAMPLES_PER_CHUNK, NUM_MEASUREMENTS_PER_SAMPLE, NUM_CHUNKS_PER_RECORDING)\n",
    "            letter_list.append(chunks)\n",
    "            np_to_txt(letter_list, fileWrite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[1, 1, 1],\n",
      "       [1, 2, 1],\n",
      "       [1, 3, 2],\n",
      "       [1, 4, 1],\n",
      "       [1, 4, 1],\n",
      "       [1, 1, 4],\n",
      "       [1, 5, 1],\n",
      "       [1, 6, 1]]), array([[1, 1, 1],\n",
      "       [1, 2, 1],\n",
      "       [1, 3, 2],\n",
      "       [1, 4, 1],\n",
      "       [1, 4, 1],\n",
      "       [1, 1, 4],\n",
      "       [1, 5, 1],\n",
      "       [1, 6, 1]])]\n",
      "success!\n"
     ]
    }
   ],
   "source": [
    "def test_chunk_data_size(chunks):\n",
    "    # confirm samples are of length NUM_SAMPLES_PER_CHUNK\n",
    "    for chunk in chunks:\n",
    "        assert chunk.shape == (NUM_SAMPLES_PER_CHUNK,NUM_MEASUREMENTS_PER_SAMPLE)\n",
    "        \n",
    "\n",
    "def test_chunk_toy_data():\n",
    "    ##################### TEST #####################\n",
    "    # description: test perfect coverage of chunk \n",
    "    data = [np.array([[0, 1], \n",
    "                     [0, 2],\n",
    "                     [1, 3],\n",
    "                     [1, 4],\n",
    "                     [0, 5],\n",
    "                     [0, 6]])]\n",
    "    # order: samples per chunk, meas per sample, chunks per recording\n",
    "    out = chunk_data(data, 2, 1, 1)\n",
    "    assert np.all(out == np.array([[3],[4]]))\n",
    "    \n",
    "    ##################### TEST #####################\n",
    "    # description: test perfect coverage of chunk, 2 recordings\n",
    "    data = [np.array([[0, 1], \n",
    "                     [0, 2],\n",
    "                     [1, 3],\n",
    "                     [1, 4],\n",
    "                     [0, 5],\n",
    "                     [0, 6]]), \n",
    "            np.array([[0, 1], \n",
    "                     [0, 2],\n",
    "                     [1, 3],\n",
    "                     [1, 4],\n",
    "                     [0, 5],\n",
    "                     [0, 6]])]\n",
    "    # order: samples per chunk, meas per sample, chunks per recording\n",
    "    out = chunk_data(data, 2, 1, 1)\n",
    "    assert np.all(out[0] == np.array([[3],[4]]))\n",
    "    assert np.all(out[1] == np.array([[3],[4]]))\n",
    "    \n",
    "    ##################### TEST #####################\n",
    "    # description: test grabbing prev/following values if button press inds < chunk size\n",
    "    data = [np.array([[0, 1], \n",
    "                     [0, 2],\n",
    "                     [1, 3],\n",
    "                     [1, 4],\n",
    "                     [0, 5],\n",
    "                     [0, 6]])]\n",
    "    out = chunk_data(data, 3, 1, 2)\n",
    "    assert np.all(out[0] == np.array([[2],[3],[4]]))\n",
    "    assert np.all(out[1] ==  np.array([[3],[4],[5]]))\n",
    "    \n",
    "    ##################### TEST #####################\n",
    "    # description: test grabbing prev with potential out of bounds error at beginning with 1 chunk\n",
    "    data = [np.array([[1, 1], \n",
    "                     [1, 2],\n",
    "                     [1, 3],\n",
    "                     [0, 4],\n",
    "                     [0, 5],\n",
    "                     [0, 6]])]\n",
    "    out = chunk_data(data, 4, 1, 1)\n",
    "    assert np.all(out[0] == np.array([[1],[2],[3],[4]]))\n",
    "    \n",
    "    ##################### TEST ####################\n",
    "    # description: test grabbing prev with potential out of bounds error at beginning with 2 chunks\n",
    "    data = [np.array([[1, 1], \n",
    "                     [1, 2],\n",
    "                     [1, 3],\n",
    "                     [0, 4],\n",
    "                     [0, 5],\n",
    "                     [0, 6]])]\n",
    "    out = chunk_data(data, 4, 1, 2)\n",
    "    assert np.all(out[0] == np.array([[1],[2],[3],[4]]))\n",
    "    assert np.all(out[1] == np.array([[1],[2],[3],[4]]))\n",
    "    \n",
    "    \n",
    "    ##################### TEST #####################\n",
    "    # description: test grabbing prev with potential out of bounds error at end with 1 chunk\n",
    "    data = [np.array([[0, 1], \n",
    "                     [0, 2],\n",
    "                     [0, 3],\n",
    "                     [0, 4],\n",
    "                     [1, 5],\n",
    "                     [1, 6]])]\n",
    "    out = chunk_data(data, 3, 1, 1)\n",
    "    assert np.all(out[0] == np.array([[4],[5],[6]]))\n",
    "    \n",
    "     ##################### TEST #####################\n",
    "    # description: test grabbing prev with potential out of bounds error at end with 2 chunks\n",
    "    data = [np.array([[0, 1], \n",
    "                     [0, 2],\n",
    "                     [0, 3],\n",
    "                     [0, 4],\n",
    "                     [1, 5],\n",
    "                     [1, 6]])]\n",
    "    out = chunk_data(data, 3, 1, 2)\n",
    "    assert np.all(out[0] == np.array([[4],[5],[6]]))\n",
    "    assert np.all(out[1] == np.array([[4],[5],[6]]))\n",
    "    \n",
    "    ##################### TEST #####################\n",
    "    # description: test full sample in recording with uneven split\n",
    "    data = [np.array([[1, 1], \n",
    "                     [1, 2],\n",
    "                     [1, 3],\n",
    "                     [1, 4],\n",
    "                     [1, 5],\n",
    "                     [1, 6],\n",
    "                     [1, 7]])]\n",
    "    \n",
    "    # num_samples_pressed >= NUM_SAMPLES_PER_CHUNK + NUM_CHUNKS_PER_RECORDING ? 7 > 2 + 3 = 5 yes\n",
    "    # step: num_samples_pressed // NUM_CHUNKS_PER_RECORDING = 7 // 3 = 2\n",
    "    out = chunk_data(data, 2, 1, 3) # step: (7-3)//2 = 2\n",
    "    assert np.all(out[0] == np.array([[1],[2]])) # first\n",
    "    assert np.all(out[1] == np.array([[6],[7]])) # middle\n",
    "    assert np.all(out[2] ==  np.array([[3],[4]])) # last\n",
    "    \n",
    "    \n",
    "    ##################### TEST #####################\n",
    "    # description: test chunk size that is greater than recording size\n",
    "    data = [np.array([[0, 1], \n",
    "                     [0, 2],\n",
    "                     [1, 3],\n",
    "                     [1, 4],\n",
    "                     [0, 5],\n",
    "                     [0, 6]])]\n",
    "    out = chunk_data(data, 8, 1, 1)\n",
    "    assert np.all(out == np.array([[1],[2],[3],[4],[5],[6],[0],[0]]))\n",
    "    \n",
    "    data = [np.array([[0,1, 1,1], \n",
    "                     [0, 1,2,1],\n",
    "                     [1, 1,3,2],\n",
    "                     [1,1, 4,1],\n",
    "                     [1,1, 4,1],\n",
    "                     [1,1,1, 4],\n",
    "                     [0, 1,5,1],\n",
    "                     [0,1, 6,1]])]\n",
    "    out = chunk_data(data, 8, 3, 1)\n",
    "    print(out)\n",
    "    \n",
    "# test_chunk_data_size(chunks)\n",
    "test_chunk_toy_data()\n",
    "print(\"success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

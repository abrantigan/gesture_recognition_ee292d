{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lauren's Data Chunking Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a list of data file paths, return one list containing recording data, each element in list \n",
    "# is an np.array containing entire recording\n",
    "def csv_to_np(data_list_name):\n",
    "    recordings = []\n",
    "    for ind, recording in enumerate(data_list_name):\n",
    "        recording_np = np.genfromtxt(recording, delimiter=',')\n",
    "        recordings.append(recording_np)\n",
    "    return recordings\n",
    "\n",
    "\n",
    "# Transfer from 10s recordings to 300, 3 data chunks\n",
    "# Each recording contributes NUM_CHUNKS_PER_RECORDING chunks to output to retain evenly distributed data\n",
    "# ASSUMPTIONS: input parameter recordings is a list of np arrays of recordings\n",
    "def chunk_data(recordings, n_samples_per_chunk, n_meas_per_samp, n_chunks_per_rec):\n",
    "    NUM_SAMPLES_PER_CHUNK = n_samples_per_chunk\n",
    "    NUM_MEASUREMENTS_PER_SAMPLE = n_meas_per_samp\n",
    "    NUM_CHUNKS_PER_RECORDING = n_chunks_per_rec \n",
    "    \n",
    "    out = []\n",
    "    for recording in recordings:\n",
    "        press_inds = np.argwhere(recording[:,0]==1).squeeze()\n",
    "        num_samples_pressed = recording[press_inds,:].shape[0]\n",
    "        \n",
    "        # CASE 0: check that we have at least 300 samples in entire recording\n",
    "        if recording.shape[0] < NUM_SAMPLES_PER_CHUNK:\n",
    "            sample_pad = np.zeros((NUM_SAMPLES_PER_CHUNK,NUM_MEASUREMENTS_PER_SAMPLE))\n",
    "            sample_pad[0:recording.shape[0],:] = recording[:,1:]\n",
    "            out.append(sample_pad)\n",
    "        \n",
    "        # CASE 1: we have enough  samples to fill out chunks with only press data\n",
    "        elif num_samples_pressed >= NUM_SAMPLES_PER_CHUNK + NUM_CHUNKS_PER_RECORDING:\n",
    "            \n",
    "            # First chunk (always capture very beginning of data)\n",
    "            first_chunk_start = press_inds[0]\n",
    "            out.append(recording[first_chunk_start:first_chunk_start+NUM_SAMPLES_PER_CHUNK,1:])\n",
    "            \n",
    "            # Last chunk (always capture very end of data)\n",
    "            last_chunk_start = press_inds[-1]-NUM_SAMPLES_PER_CHUNK+1\n",
    "            out.append(recording[last_chunk_start:last_chunk_start+NUM_SAMPLES_PER_CHUNK,1:])\n",
    "            \n",
    "            # Middle chunks (evenly space from first chunk for simplicity)\n",
    "            step = num_samples_pressed // NUM_CHUNKS_PER_RECORDING\n",
    "            prev_chunk_start = first_chunk_start\n",
    "            for i in range(NUM_CHUNKS_PER_RECORDING-2):\n",
    "                next_chunk_start = prev_chunk_start+step\n",
    "                out.append(recording[next_chunk_start:next_chunk_start+NUM_SAMPLES_PER_CHUNK,1:])\n",
    "                prev_chunk_start = next_chunk_start\n",
    "                \n",
    "        # CASE 2: We don't have enough samples to fill chunks: need to pull from surrounding data\n",
    "        elif num_samples_pressed < NUM_SAMPLES_PER_CHUNK + NUM_CHUNKS_PER_RECORDING:\n",
    "            # First chunk: captures until end of recording, starting however many samples before needed to do that\n",
    "            first_chunk_start = max(0, press_inds[-1]-NUM_SAMPLES_PER_CHUNK+1)\n",
    "            out.append(recording[first_chunk_start:first_chunk_start+NUM_SAMPLES_PER_CHUNK,1:])\n",
    "            \n",
    "            # Last chunk: captures from start of recording, going however many samples after needed to do that\n",
    "            last_chunk_end = min(recording.shape[0], press_inds[0]+NUM_SAMPLES_PER_CHUNK)\n",
    "            out.append(recording[last_chunk_end-NUM_SAMPLES_PER_CHUNK:last_chunk_end,1:])\n",
    "            \n",
    "            # Middle chunks: (evenly space from first chunk for simplicity)\n",
    "            step = (last_chunk_end - first_chunk_start+1) // NUM_CHUNKS_PER_RECORDING\n",
    "            prev_chunk_start = first_chunk_start\n",
    "            for i in range(NUM_CHUNKS_PER_RECORDING-2):\n",
    "                next_chunk_start = prev_chunk_start+step\n",
    "                out.append(recording[next_chunk_start:next_chunk_start+NUM_SAMPLES_PER_CHUNK,1:])\n",
    "                prev_chunk_start = next_chunk_start\n",
    "                \n",
    "    return out\n",
    "\n",
    "## for every name and every letter\n",
    "## output_A_name\n",
    "\n",
    "# # Note: train obtained from running Splitting_up_data.ipynb on raw training data\n",
    "# train = ['Data/A_lauren_11.csv', 'Data/A_lauren_18.csv', 'Data/A_lauren_20.csv', 'Data/B_lauren_14.csv',\\\n",
    "#          'Data/B_lauren_15.csv', 'Data/B_lauren_17.csv', 'Data/C_lauren_28.csv', 'Data/C_lauren_18.csv', \\\n",
    "#          'Data/C_lauren_3.csv', 'Data/D_lauren_8.csv', 'Data/D_lauren_23.csv', 'Data/D_lauren_4.csv',\\\n",
    "#          'Data/E_lauren_5.csv', 'Data/E_lauren_7.csv', 'Data/E_lauren_25.csv', 'Data/F_lauren_4.csv', \\\n",
    "#          'Data/F_lauren_26.csv', 'Data/F_lauren_2.csv', 'Data/G_lauren_4.csv', 'Data/G_lauren_8.csv',\\\n",
    "#          'Data/G_lauren_13.csv', 'Data/H_lauren_29.csv', 'Data/H_lauren_23.csv', 'Data/H_lauren_22.csv', \\\n",
    "#          'Data/I_lauren_19.csv', 'Data/I_lauren_22.csv', 'Data/I_lauren_8.csv', 'Data/J_lauren_1.csv', \\\n",
    "#          'Data/J_lauren_16.csv', 'Data/J_lauren_23.csv', 'Data/K_lauren_1.csv', 'Data/K_lauren_10.csv', \\\n",
    "#          'Data/K_lauren_23.csv', 'Data/L_lauren_30.csv', 'Data/L_lauren_6.csv', 'Data/L_lauren_26.csv', \\\n",
    "#          'Data/M_lauren_21.csv', 'Data/M_lauren_3.csv', 'Data/M_lauren_19.csv', 'Data/N_lauren_1.csv', \\\n",
    "#          'Data/N_lauren_14.csv', 'Data/N_lauren_25.csv', 'Data/O_lauren_17.csv', 'Data/O_lauren_10.csv', \\\n",
    "#          'Data/O_lauren_28.csv', 'Data/P_lauren_22.csv', 'Data/P_lauren_29.csv', 'Data/P_lauren_3.csv', \\\n",
    "#          'Data/Q_lauren_10.csv', 'Data/Q_lauren_29.csv', 'Data/Q_lauren_23.csv', 'Data/R_lauren_4.csv', \\\n",
    "#          'Data/R_lauren_8.csv', 'Data/R_lauren_12.csv', 'Data/S_lauren_27.csv', 'Data/S_lauren_12.csv', \\\n",
    "#          'Data/S_lauren_18.csv', 'Data/T_lauren_9.csv', 'Data/T_lauren_21.csv', 'Data/T_lauren_7.csv', \\\n",
    "#          'Data/U_lauren_11.csv', 'Data/U_lauren_24.csv', 'Data/U_lauren_3.csv', 'Data/V_lauren_5.csv', \\\n",
    "#          'Data/V_lauren_26.csv', 'Data/V_lauren_10.csv', 'Data/W_lauren_19.csv', 'Data/W_lauren_2.csv', \\\n",
    "#          'Data/W_lauren_18.csv', 'Data/X_lauren_2.csv', 'Data/X_lauren_28.csv', 'Data/X_lauren_8.csv', \\\n",
    "#          'Data/Y_lauren_29.csv', 'Data/Y_lauren_3.csv', 'Data/Y_lauren_17.csv', 'Data/Z_lauren_8.csv', \\\n",
    "#          'Data/Z_lauren_20.csv', 'Data/Z_lauren_23.csv']\n",
    "# recordings = csv_to_np(train)\n",
    "# assert len(recordings) == len(train)\n",
    "# NUM_SAMPLES_PER_CHUNK = 300\n",
    "# NUM_MEASUREMENTS_PER_SAMPLE = 3\n",
    "# NUM_CHUNKS_PER_RECORDING = 5\n",
    "# chunks = chunk_data(recordings, NUM_SAMPLES_PER_CHUNK, NUM_MEASUREMENTS_PER_SAMPLE, NUM_CHUNKS_PER_RECORDING)\n",
    "# print(\"executed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Katherine's Data Zero Padding,with max length 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a list of data file paths, return one list containing recording data, each element in list \n",
    "# is an np.array containing entire recording\n",
    "def csv_to_np(data_list_name):\n",
    "    recordings = []\n",
    "    for ind, recording in enumerate(data_list_name):\n",
    "        recording_np = np.genfromtxt(recording, delimiter=',')\n",
    "        recordings.append(recording_np)\n",
    "    return recordings\n",
    "\n",
    "\n",
    "# Transfer from 10s recordings to 300, 3 data chunks\n",
    "# Each recording contributes NUM_CHUNKS_PER_RECORDING chunks to output to retain evenly distributed data\n",
    "# ASSUMPTIONS: input parameter recordings is a list of np arrays of recordings\n",
    "def zp_data(rec, chunk_length):\n",
    "    recording = np.squeeze(np.asarray(rec))\n",
    "    press_inds = np.argwhere(recording[:,0]==1).squeeze()\n",
    "    sample = recording[press_inds,1:4];\n",
    "    num_samples_pressed = recording[press_inds,:].shape[0]\n",
    "    out = np.zeros((chunk_length,3))\n",
    "    # CASE 0: check that we have at least 300 samples in entire recording\n",
    "    if num_samples_pressed <= chunk_length:\n",
    "        out[0:num_samples_pressed,:] = sample\n",
    "    else:\n",
    "        out[0:chunk_length,:] = sample[0:chunk_length,:]\n",
    "    return [out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#given list of arrays, write to text folder/file.txt\n",
    "def np_to_txt(chunks, fileName):\n",
    "    with open(fileName, \"a+\") as f:\n",
    "        for i in range(len(chunks)): #10\n",
    "            for j in range(len(chunks[i])): #400 samples\n",
    "                f.write('\\n\\n-,-,-\\n')\n",
    "#                 print(chunks[i][j].shape)\n",
    "                for k in range(chunks[i][j].shape[0]):\n",
    "#                     print(chunks[i][j][k])\n",
    "                    np.savetxt(f, [chunks[i][j][k]], fmt=\"%3.3f\", delimiter=',', newline='')\n",
    "                    f.write('\\n')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing: A_katherine.txt\n",
      "Writing: A_chris.txt\n",
      "Writing: A_annie.txt\n",
      "Writing: A_lauren.txt\n",
      "Writing: A_hallie.txt\n",
      "Writing: B_katherine.txt\n",
      "Writing: B_chris.txt\n",
      "Writing: B_annie.txt\n",
      "Writing: B_lauren.txt\n",
      "Writing: B_hallie.txt\n",
      "Writing: C_katherine.txt\n",
      "Writing: C_chris.txt\n",
      "Writing: C_annie.txt\n",
      "Writing: C_lauren.txt\n",
      "Writing: C_hallie.txt\n",
      "Writing: D_katherine.txt\n",
      "Writing: D_chris.txt\n",
      "Writing: D_annie.txt\n",
      "Writing: D_lauren.txt\n",
      "Writing: D_hallie.txt\n",
      "Writing: E_katherine.txt\n",
      "Writing: E_chris.txt\n",
      "Writing: E_annie.txt\n",
      "Writing: E_lauren.txt\n",
      "Writing: E_hallie.txt\n",
      "Writing: F_katherine.txt\n",
      "Writing: F_chris.txt\n",
      "Writing: F_annie.txt\n",
      "Writing: F_lauren.txt\n",
      "Writing: F_hallie.txt\n",
      "Writing: G_katherine.txt\n",
      "Writing: G_chris.txt\n",
      "Writing: G_annie.txt\n",
      "Writing: G_lauren.txt\n",
      "Writing: G_hallie.txt\n",
      "Writing: H_katherine.txt\n",
      "Writing: H_chris.txt\n",
      "Writing: H_annie.txt\n",
      "Writing: H_lauren.txt\n",
      "Writing: H_hallie.txt\n",
      "Writing: I_katherine.txt\n",
      "Writing: I_chris.txt\n",
      "Writing: I_annie.txt\n",
      "Writing: I_lauren.txt\n",
      "Writing: I_hallie.txt\n",
      "Writing: J_katherine.txt\n",
      "Writing: J_chris.txt\n",
      "Writing: J_annie.txt\n",
      "Writing: J_lauren.txt\n",
      "Writing: J_hallie.txt\n",
      "Writing: K_katherine.txt\n",
      "Writing: K_chris.txt\n",
      "Writing: K_annie.txt\n",
      "Writing: K_lauren.txt\n",
      "Writing: K_hallie.txt\n",
      "Writing: L_katherine.txt\n",
      "Writing: L_chris.txt\n",
      "Writing: L_annie.txt\n",
      "Writing: L_lauren.txt\n",
      "Writing: L_hallie.txt\n",
      "Writing: M_katherine.txt\n",
      "Writing: M_chris.txt\n",
      "Writing: M_annie.txt\n",
      "Writing: M_lauren.txt\n",
      "Writing: M_hallie.txt\n",
      "Writing: N_katherine.txt\n",
      "Writing: N_chris.txt\n",
      "Writing: N_annie.txt\n",
      "Writing: N_lauren.txt\n",
      "Writing: N_hallie.txt\n",
      "Writing: O_katherine.txt\n",
      "Writing: O_chris.txt\n",
      "Writing: O_annie.txt\n",
      "Writing: O_lauren.txt\n",
      "Writing: O_hallie.txt\n",
      "Writing: P_katherine.txt\n",
      "Writing: P_chris.txt\n",
      "Writing: P_annie.txt\n",
      "Writing: P_lauren.txt\n",
      "Writing: P_hallie.txt\n",
      "Writing: Q_katherine.txt\n",
      "Writing: Q_chris.txt\n",
      "Writing: Q_annie.txt\n",
      "Writing: Q_lauren.txt\n",
      "Writing: Q_hallie.txt\n",
      "Writing: R_katherine.txt\n",
      "Writing: R_chris.txt\n",
      "Writing: R_annie.txt\n",
      "Writing: R_lauren.txt\n",
      "Writing: R_hallie.txt\n",
      "Writing: S_katherine.txt\n",
      "Writing: S_chris.txt\n",
      "Writing: S_annie.txt\n",
      "Writing: S_lauren.txt\n",
      "Writing: S_hallie.txt\n",
      "Writing: T_katherine.txt\n",
      "Writing: T_chris.txt\n",
      "Writing: T_annie.txt\n",
      "Writing: T_lauren.txt\n",
      "Writing: T_hallie.txt\n",
      "Writing: U_katherine.txt\n",
      "Writing: U_chris.txt\n",
      "Writing: U_annie.txt\n",
      "Writing: U_lauren.txt\n",
      "Writing: U_hallie.txt\n",
      "Writing: V_katherine.txt\n",
      "Writing: V_chris.txt\n",
      "Writing: V_annie.txt\n",
      "Writing: V_lauren.txt\n",
      "Writing: V_hallie.txt\n",
      "Writing: W_katherine.txt\n",
      "Writing: W_chris.txt\n",
      "Writing: W_annie.txt\n",
      "Writing: W_lauren.txt\n",
      "Writing: W_hallie.txt\n",
      "Writing: X_katherine.txt\n",
      "Writing: X_chris.txt\n",
      "Writing: X_annie.txt\n",
      "Writing: X_lauren.txt\n",
      "Writing: X_hallie.txt\n",
      "Writing: Y_katherine.txt\n",
      "Writing: Y_chris.txt\n",
      "Writing: Y_annie.txt\n",
      "Writing: Y_lauren.txt\n",
      "Writing: Y_hallie.txt\n",
      "Writing: Z_katherine.txt\n",
      "Writing: Z_chris.txt\n",
      "Writing: Z_annie.txt\n",
      "Writing: Z_lauren.txt\n",
      "Writing: Z_hallie.txt\n",
      "Writing: apostrophe_katherine.txt\n",
      "Writing: apostrophe_chris.txt\n",
      "Writing: apostrophe_annie.txt\n",
      "Writing: apostrophe_lauren.txt\n",
      "Writing: apostrophe_hallie.txt\n",
      "Writing: backspace_katherine.txt\n",
      "Writing: backspace_chris.txt\n",
      "Writing: backspace_annie.txt\n",
      "Writing: backspace_lauren.txt\n",
      "Writing: backspace_hallie.txt\n",
      "Writing: comma_katherine.txt\n",
      "Writing: comma_chris.txt\n",
      "Writing: comma_annie.txt\n",
      "Writing: comma_lauren.txt\n",
      "Writing: comma_hallie.txt\n",
      "Writing: done_katherine.txt\n",
      "Writing: done_chris.txt\n",
      "Writing: done_annie.txt\n",
      "Writing: done_lauren.txt\n",
      "Writing: done_hallie.txt\n",
      "Writing: exclamation_point_katherine.txt\n",
      "Writing: exclamation_point_chris.txt\n",
      "Writing: exclamation_point_annie.txt\n",
      "Writing: exclamation_point_lauren.txt\n",
      "Writing: exclamation_point_hallie.txt\n",
      "Writing: period_katherine.txt\n",
      "Writing: period_chris.txt\n",
      "Writing: period_annie.txt\n",
      "Writing: period_lauren.txt\n",
      "Writing: period_hallie.txt\n",
      "Writing: question_mark_katherine.txt\n",
      "Writing: question_mark_chris.txt\n",
      "Writing: question_mark_annie.txt\n",
      "Writing: question_mark_lauren.txt\n",
      "Writing: question_mark_hallie.txt\n",
      "Writing: quotes_katherine.txt\n",
      "Writing: quotes_chris.txt\n",
      "Writing: quotes_annie.txt\n",
      "Writing: quotes_lauren.txt\n",
      "Writing: quotes_hallie.txt\n",
      "Writing: slash_katherine.txt\n",
      "Writing: slash_chris.txt\n",
      "Writing: slash_annie.txt\n",
      "Writing: slash_lauren.txt\n",
      "Writing: slash_hallie.txt\n",
      "Writing: space_katherine.txt\n",
      "Writing: space_chris.txt\n",
      "Writing: space_annie.txt\n",
      "Writing: space_lauren.txt\n",
      "Writing: space_hallie.txt\n",
      "Writing: negative_katherine.txt\n",
      "Writing: negative_chris.txt\n",
      "Writing: negative_annie.txt\n",
      "Writing: negative_lauren.txt\n",
      "Writing: negative_hallie.txt\n"
     ]
    }
   ],
   "source": [
    "############################################################################################################################\n",
    "chunk_length=400;\n",
    "\n",
    "path = r\"D:\\292d\\letters\"#r\"C:\\Users\\kathe\\Documents\\GitHub\\tensorflow\\tensorflow\\lite\\micro\\examples\\magic_wand\\train\\letters\"\n",
    "write_path = r\"D:\\292d\\data\"#r\"C:\\Users\\kathe\\Documents\\GitHub\\tensorflow\\tensorflow\\lite\\micro\\examples\\magic_wand\\train\\data\"\n",
    "names=[\"katherine\",\"chris\",\"annie\", \"lauren\",\"hallie\"]\n",
    "letters=[\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\",\n",
    "          \"apostrophe\",\"backspace\",\"comma\",\"done\",\"exclamation_point\", \"period\",\"question_mark\",\"quotes\",\"slash\",\"space\",\"negative\"]\n",
    "for l in letters: \n",
    "    for n in names:\n",
    "        letter_list =[]\n",
    "        fileWrite =write_path+'\\\\'+l+'\\output_'+l+'_'+n+'.txt'  \n",
    "        open(fileWrite, 'w').close()\n",
    "        print('Writing: '+l+'_'+n+'.txt')       \n",
    "        for file in glob.glob(path+'\\\\'+l+'\\\\'+l+'_'+n +'_'+'*.csv'):\n",
    "            ##for all lauren's As, write -,-,-  write list of arrays to .txt file (output_A_lauren)\n",
    "            recordings = csv_to_np([file])\n",
    "            press_inds = np.argwhere(recording[:,0]==1).squeeze()\n",
    "            chunk = zp_data(recordings, chunk_length)\n",
    "            letter_list.append(chunk)\n",
    "        np_to_txt(letter_list, fileWrite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Histogram of Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing: A_lauren.txt\n",
      "Writing: A_katherine.txt\n",
      "Writing: A_annie.txt\n",
      "Writing: A_chris.txt\n",
      "Writing: A_hallie.txt\n",
      "Writing: B_lauren.txt\n",
      "Writing: B_katherine.txt\n",
      "Writing: B_annie.txt\n",
      "Writing: B_chris.txt\n",
      "Writing: B_hallie.txt\n",
      "Writing: C_lauren.txt\n",
      "Writing: C_katherine.txt\n",
      "Writing: C_annie.txt\n",
      "Writing: C_chris.txt\n",
      "Writing: C_hallie.txt\n",
      "Writing: D_lauren.txt\n",
      "Writing: D_katherine.txt\n",
      "Writing: D_annie.txt\n",
      "Writing: D_chris.txt\n",
      "Writing: D_hallie.txt\n",
      "Writing: E_lauren.txt\n",
      "Writing: E_katherine.txt\n",
      "Writing: E_annie.txt\n",
      "Writing: E_chris.txt\n",
      "Writing: E_hallie.txt\n",
      "Writing: F_lauren.txt\n",
      "Writing: F_katherine.txt\n",
      "Writing: F_annie.txt\n",
      "Writing: F_chris.txt\n",
      "Writing: F_hallie.txt\n",
      "Writing: G_lauren.txt\n",
      "Writing: G_katherine.txt\n",
      "Writing: G_annie.txt\n",
      "Writing: G_chris.txt\n",
      "Writing: G_hallie.txt\n",
      "Writing: H_lauren.txt\n",
      "Writing: H_katherine.txt\n",
      "Writing: H_annie.txt\n",
      "Writing: H_chris.txt\n",
      "Writing: H_hallie.txt\n",
      "Writing: I_lauren.txt\n",
      "Writing: I_katherine.txt\n",
      "Writing: I_annie.txt\n",
      "Writing: I_chris.txt\n",
      "Writing: I_hallie.txt\n",
      "Writing: J_lauren.txt\n",
      "Writing: J_katherine.txt\n",
      "Writing: J_annie.txt\n",
      "Writing: J_chris.txt\n",
      "Writing: J_hallie.txt\n",
      "Writing: K_lauren.txt\n",
      "Writing: K_katherine.txt\n",
      "Writing: K_annie.txt\n",
      "Writing: K_chris.txt\n",
      "Writing: K_hallie.txt\n",
      "Writing: L_lauren.txt\n",
      "Writing: L_katherine.txt\n",
      "Writing: L_annie.txt\n",
      "Writing: L_chris.txt\n",
      "Writing: L_hallie.txt\n",
      "Writing: M_lauren.txt\n",
      "Writing: M_katherine.txt\n",
      "Writing: M_annie.txt\n",
      "Writing: M_chris.txt\n",
      "Writing: M_hallie.txt\n",
      "Writing: N_lauren.txt\n",
      "Writing: N_katherine.txt\n",
      "Writing: N_annie.txt\n",
      "Writing: N_chris.txt\n",
      "Writing: N_hallie.txt\n",
      "Writing: O_lauren.txt\n",
      "Writing: O_katherine.txt\n",
      "Writing: O_annie.txt\n",
      "Writing: O_chris.txt\n",
      "Writing: O_hallie.txt\n",
      "Writing: P_lauren.txt\n",
      "Writing: P_katherine.txt\n",
      "Writing: P_annie.txt\n",
      "Writing: P_chris.txt\n",
      "Writing: P_hallie.txt\n",
      "Writing: Q_lauren.txt\n",
      "Writing: Q_katherine.txt\n",
      "Writing: Q_annie.txt\n",
      "Writing: Q_chris.txt\n",
      "Writing: Q_hallie.txt\n",
      "Writing: R_lauren.txt\n",
      "Writing: R_katherine.txt\n",
      "Writing: R_annie.txt\n",
      "Writing: R_chris.txt\n",
      "Writing: R_hallie.txt\n",
      "Writing: S_lauren.txt\n",
      "Writing: S_katherine.txt\n",
      "Writing: S_annie.txt\n",
      "Writing: S_chris.txt\n",
      "Writing: S_hallie.txt\n",
      "Writing: T_lauren.txt\n",
      "Writing: T_katherine.txt\n",
      "Writing: T_annie.txt\n",
      "Writing: T_chris.txt\n",
      "Writing: T_hallie.txt\n",
      "Writing: U_lauren.txt\n",
      "Writing: U_katherine.txt\n",
      "Writing: U_annie.txt\n",
      "Writing: U_chris.txt\n",
      "Writing: U_hallie.txt\n",
      "Writing: V_lauren.txt\n",
      "Writing: V_katherine.txt\n",
      "Writing: V_annie.txt\n",
      "Writing: V_chris.txt\n",
      "Writing: V_hallie.txt\n",
      "Writing: W_lauren.txt\n",
      "Writing: W_katherine.txt\n",
      "Writing: W_annie.txt\n",
      "Writing: W_chris.txt\n",
      "Writing: W_hallie.txt\n",
      "Writing: X_lauren.txt\n",
      "Writing: X_katherine.txt\n",
      "Writing: X_annie.txt\n",
      "Writing: X_chris.txt\n",
      "Writing: X_hallie.txt\n",
      "Writing: Y_lauren.txt\n",
      "Writing: Y_katherine.txt\n",
      "Writing: Y_annie.txt\n",
      "Writing: Y_chris.txt\n",
      "Writing: Y_hallie.txt\n",
      "Writing: Z_lauren.txt\n",
      "Writing: Z_katherine.txt\n",
      "Writing: Z_annie.txt\n",
      "Writing: Z_chris.txt\n",
      "Writing: Z_hallie.txt\n",
      "Writing: apostrophe_lauren.txt\n",
      "Writing: apostrophe_katherine.txt\n",
      "Writing: apostrophe_annie.txt\n",
      "Writing: apostrophe_chris.txt\n",
      "Writing: apostrophe_hallie.txt\n",
      "Writing: backspace_lauren.txt\n",
      "Writing: backspace_katherine.txt\n",
      "Writing: backspace_annie.txt\n",
      "Writing: backspace_chris.txt\n",
      "Writing: backspace_hallie.txt\n",
      "Writing: comma_lauren.txt\n",
      "Writing: comma_katherine.txt\n",
      "Writing: comma_annie.txt\n",
      "Writing: comma_chris.txt\n",
      "Writing: comma_hallie.txt\n",
      "Writing: done_lauren.txt\n",
      "Writing: done_katherine.txt\n",
      "Writing: done_annie.txt\n",
      "Writing: done_chris.txt\n",
      "Writing: done_hallie.txt\n",
      "Writing: exclamation_point_lauren.txt\n",
      "Writing: exclamation_point_katherine.txt\n",
      "Writing: exclamation_point_annie.txt\n",
      "Writing: exclamation_point_chris.txt\n",
      "Writing: exclamation_point_hallie.txt\n",
      "Writing: period_lauren.txt\n",
      "Writing: period_katherine.txt\n",
      "Writing: period_annie.txt\n",
      "Writing: period_chris.txt\n",
      "Writing: period_hallie.txt\n",
      "Writing: question_mark_lauren.txt\n",
      "Writing: question_mark_katherine.txt\n",
      "Writing: question_mark_annie.txt\n",
      "Writing: question_mark_chris.txt\n",
      "Writing: question_mark_hallie.txt\n",
      "Writing: quotes_lauren.txt\n",
      "Writing: quotes_katherine.txt\n",
      "Writing: quotes_annie.txt\n",
      "Writing: quotes_chris.txt\n",
      "Writing: quotes_hallie.txt\n",
      "Writing: slash_lauren.txt\n",
      "Writing: slash_katherine.txt\n",
      "Writing: slash_annie.txt\n",
      "Writing: slash_chris.txt\n",
      "Writing: slash_hallie.txt\n",
      "Writing: space_lauren.txt\n",
      "Writing: space_katherine.txt\n",
      "Writing: space_annie.txt\n",
      "Writing: space_chris.txt\n",
      "Writing: space_hallie.txt\n",
      "Writing: negative_lauren.txt\n",
      "Writing: negative_katherine.txt\n",
      "Writing: negative_annie.txt\n",
      "Writing: negative_chris.txt\n",
      "Writing: negative_hallie.txt\n"
     ]
    }
   ],
   "source": [
    "############################################################################################################################\n",
    "chunk_length=500;\n",
    "\n",
    "path = r\"D:\\292d\\letters\"#r\"C:\\Users\\kathe\\Documents\\GitHub\\tensorflow\\tensorflow\\lite\\micro\\examples\\magic_wand\\train\\letters\"\n",
    "write_path = r\"D:\\292d\\data\"#r\"C:\\Users\\kathe\\Documents\\GitHub\\tensorflow\\tensorflow\\lite\\micro\\examples\\magic_wand\\train\\data\"\n",
    "names=[\"lauren\",\"katherine\",\"annie\", \"chris\",\"hallie\"]\n",
    "letters=[\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\",\n",
    "          \"apostrophe\",\"backspace\",\"comma\",\"done\",\"exclamation_point\", \"period\",\"question_mark\",\"quotes\",\"slash\",\"space\",\"negative\"]\n",
    "lengths=[]\n",
    "for l in letters: \n",
    "    for n in names:\n",
    "        letter_list =[]\n",
    "        fileWrite =write_path+'\\\\'+l+'\\output_'+l+'_'+n+'.txt'  \n",
    "        open(fileWrite, 'w').close()\n",
    "        print('Writing: '+l+'_'+n+'.txt')       \n",
    "        for file in glob.glob(path+'\\\\'+l+'\\\\'+l+'_'+n +'_'+'*.csv'):\n",
    "            ##for all lauren's As, write -,-,-  write list of arrays to .txt file (output_A_lauren)\n",
    "            recording = np.squeeze(np.asarray(csv_to_np([file])))\n",
    "            press_inds = np.argwhere(recording[:,0]==1).squeeze()\n",
    "            lengths.append(len(press_inds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 18.,  54.,  82., 122., 152., 177., 180., 169., 196., 205., 120.,\n",
       "        111., 106., 111., 110., 102., 130., 135., 119.,  87.,  88.,  55.,\n",
       "         47.,  42.,  27.,  34.,  30.,  23.,  18.,  18.,  13.,  11.,  11.,\n",
       "          8.,  10.,   5.,   1.,   5.,   4.,   6.,   0.,   0.,   1.,   1.,\n",
       "          0.,   0.,   0.,   0.,   0.,   1.]),\n",
       " array([ 16.  ,  31.44,  46.88,  62.32,  77.76,  93.2 , 108.64, 124.08,\n",
       "        139.52, 154.96, 170.4 , 185.84, 201.28, 216.72, 232.16, 247.6 ,\n",
       "        263.04, 278.48, 293.92, 309.36, 324.8 , 340.24, 355.68, 371.12,\n",
       "        386.56, 402.  , 417.44, 432.88, 448.32, 463.76, 479.2 , 494.64,\n",
       "        510.08, 525.52, 540.96, 556.4 , 571.84, 587.28, 602.72, 618.16,\n",
       "        633.6 , 649.04, 664.48, 679.92, 695.36, 710.8 , 726.24, 741.68,\n",
       "        757.12, 772.56, 788.  ]),\n",
       " <a list of 50 Patch objects>)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(lengths,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[1, 1, 1],\n",
      "       [1, 2, 1],\n",
      "       [1, 3, 2],\n",
      "       [1, 4, 1],\n",
      "       [1, 4, 1],\n",
      "       [1, 1, 4],\n",
      "       [1, 5, 1],\n",
      "       [1, 6, 1]]), array([[1, 1, 1],\n",
      "       [1, 2, 1],\n",
      "       [1, 3, 2],\n",
      "       [1, 4, 1],\n",
      "       [1, 4, 1],\n",
      "       [1, 1, 4],\n",
      "       [1, 5, 1],\n",
      "       [1, 6, 1]])]\n",
      "success!\n"
     ]
    }
   ],
   "source": [
    "def test_chunk_data_size(chunks):\n",
    "    # confirm samples are of length NUM_SAMPLES_PER_CHUNK\n",
    "    for chunk in chunks:\n",
    "        assert chunk.shape == (NUM_SAMPLES_PER_CHUNK,NUM_MEASUREMENTS_PER_SAMPLE)\n",
    "        \n",
    "\n",
    "def test_chunk_toy_data():\n",
    "    ##################### TEST #####################\n",
    "    # description: test perfect coverage of chunk \n",
    "    data = [np.array([[0, 1], \n",
    "                     [0, 2],\n",
    "                     [1, 3],\n",
    "                     [1, 4],\n",
    "                     [0, 5],\n",
    "                     [0, 6]])]\n",
    "    # order: samples per chunk, meas per sample, chunks per recording\n",
    "    out = chunk_data(data, 2, 1, 1)\n",
    "    assert np.all(out == np.array([[3],[4]]))\n",
    "    \n",
    "    ##################### TEST #####################\n",
    "    # description: test perfect coverage of chunk, 2 recordings\n",
    "    data = [np.array([[0, 1], \n",
    "                     [0, 2],\n",
    "                     [1, 3],\n",
    "                     [1, 4],\n",
    "                     [0, 5],\n",
    "                     [0, 6]]), \n",
    "            np.array([[0, 1], \n",
    "                     [0, 2],\n",
    "                     [1, 3],\n",
    "                     [1, 4],\n",
    "                     [0, 5],\n",
    "                     [0, 6]])]\n",
    "    # order: samples per chunk, meas per sample, chunks per recording\n",
    "    out = chunk_data(data, 2, 1, 1)\n",
    "    assert np.all(out[0] == np.array([[3],[4]]))\n",
    "    assert np.all(out[1] == np.array([[3],[4]]))\n",
    "    \n",
    "    ##################### TEST #####################\n",
    "    # description: test grabbing prev/following values if button press inds < chunk size\n",
    "    data = [np.array([[0, 1], \n",
    "                     [0, 2],\n",
    "                     [1, 3],\n",
    "                     [1, 4],\n",
    "                     [0, 5],\n",
    "                     [0, 6]])]\n",
    "    out = chunk_data(data, 3, 1, 2)\n",
    "    assert np.all(out[0] == np.array([[2],[3],[4]]))\n",
    "    assert np.all(out[1] ==  np.array([[3],[4],[5]]))\n",
    "    \n",
    "    ##################### TEST #####################\n",
    "    # description: test grabbing prev with potential out of bounds error at beginning with 1 chunk\n",
    "    data = [np.array([[1, 1], \n",
    "                     [1, 2],\n",
    "                     [1, 3],\n",
    "                     [0, 4],\n",
    "                     [0, 5],\n",
    "                     [0, 6]])]\n",
    "    out = chunk_data(data, 4, 1, 1)\n",
    "    assert np.all(out[0] == np.array([[1],[2],[3],[4]]))\n",
    "    \n",
    "    ##################### TEST ####################\n",
    "    # description: test grabbing prev with potential out of bounds error at beginning with 2 chunks\n",
    "    data = [np.array([[1, 1], \n",
    "                     [1, 2],\n",
    "                     [1, 3],\n",
    "                     [0, 4],\n",
    "                     [0, 5],\n",
    "                     [0, 6]])]\n",
    "    out = chunk_data(data, 4, 1, 2)\n",
    "    assert np.all(out[0] == np.array([[1],[2],[3],[4]]))\n",
    "    assert np.all(out[1] == np.array([[1],[2],[3],[4]]))\n",
    "    \n",
    "    \n",
    "    ##################### TEST #####################\n",
    "    # description: test grabbing prev with potential out of bounds error at end with 1 chunk\n",
    "    data = [np.array([[0, 1], \n",
    "                     [0, 2],\n",
    "                     [0, 3],\n",
    "                     [0, 4],\n",
    "                     [1, 5],\n",
    "                     [1, 6]])]\n",
    "    out = chunk_data(data, 3, 1, 1)\n",
    "    assert np.all(out[0] == np.array([[4],[5],[6]]))\n",
    "    \n",
    "     ##################### TEST #####################\n",
    "    # description: test grabbing prev with potential out of bounds error at end with 2 chunks\n",
    "    data = [np.array([[0, 1], \n",
    "                     [0, 2],\n",
    "                     [0, 3],\n",
    "                     [0, 4],\n",
    "                     [1, 5],\n",
    "                     [1, 6]])]\n",
    "    out = chunk_data(data, 3, 1, 2)\n",
    "    assert np.all(out[0] == np.array([[4],[5],[6]]))\n",
    "    assert np.all(out[1] == np.array([[4],[5],[6]]))\n",
    "    \n",
    "    ##################### TEST #####################\n",
    "    # description: test full sample in recording with uneven split\n",
    "    data = [np.array([[1, 1], \n",
    "                     [1, 2],\n",
    "                     [1, 3],\n",
    "                     [1, 4],\n",
    "                     [1, 5],\n",
    "                     [1, 6],\n",
    "                     [1, 7]])]\n",
    "    \n",
    "    # num_samples_pressed >= NUM_SAMPLES_PER_CHUNK + NUM_CHUNKS_PER_RECORDING ? 7 > 2 + 3 = 5 yes\n",
    "    # step: num_samples_pressed // NUM_CHUNKS_PER_RECORDING = 7 // 3 = 2\n",
    "    out = chunk_data(data, 2, 1, 3) # step: (7-3)//2 = 2\n",
    "    assert np.all(out[0] == np.array([[1],[2]])) # first\n",
    "    assert np.all(out[1] == np.array([[6],[7]])) # middle\n",
    "    assert np.all(out[2] ==  np.array([[3],[4]])) # last\n",
    "    \n",
    "    \n",
    "    ##################### TEST #####################\n",
    "    # description: test chunk size that is greater than recording size\n",
    "    data = [np.array([[0, 1], \n",
    "                     [0, 2],\n",
    "                     [1, 3],\n",
    "                     [1, 4],\n",
    "                     [0, 5],\n",
    "                     [0, 6]])]\n",
    "    out = chunk_data(data, 8, 1, 1)\n",
    "    assert np.all(out == np.array([[1],[2],[3],[4],[5],[6],[0],[0]]))\n",
    "    \n",
    "    data = [np.array([[0,1, 1,1], \n",
    "                     [0, 1,2,1],\n",
    "                     [1, 1,3,2],\n",
    "                     [1,1, 4,1],\n",
    "                     [1,1, 4,1],\n",
    "                     [1,1,1, 4],\n",
    "                     [0, 1,5,1],\n",
    "                     [0,1, 6,1]])]\n",
    "    out = chunk_data(data, 8, 3, 1)\n",
    "    print(out)\n",
    "    \n",
    "# test_chunk_data_size(chunks)\n",
    "test_chunk_toy_data()\n",
    "print(\"success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

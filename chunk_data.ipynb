{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executed\n"
     ]
    }
   ],
   "source": [
    "# given a list of data file paths, return one list containing recording data, each element in list \n",
    "# is an np.array containing entire recording\n",
    "def csv_to_np(data_list_name):\n",
    "    recordings = []\n",
    "    for ind, recording in enumerate(data_list_name):\n",
    "        recording_np = np.genfromtxt(recording, delimiter=',')\n",
    "        recordings.append(recording_np)\n",
    "    return recordings\n",
    "\n",
    "\n",
    "# Transfer from 10s recordings to 300, 3 data chunks\n",
    "# Each recording contributes NUM_CHUNKS_PER_RECORDING chunks to output to retain evenly distributed data\n",
    "# ASSUMPTIONS: input parameter recordings is a list of np arrays of recordings\n",
    "def chunk_data(recordings, n_samples_per_chunk, n_meas_per_samp, n_chunks_per_rec):\n",
    "    NUM_SAMPLES_PER_CHUNK = n_samples_per_chunk\n",
    "    NUM_MEASUREMENTS_PER_SAMPLE = n_meas_per_samp\n",
    "    NUM_CHUNKS_PER_RECORDING = n_chunks_per_rec \n",
    "    \n",
    "    out = []\n",
    "    for recording in recordings:\n",
    "        press_inds = np.argwhere(recording[:,0]==1).squeeze()\n",
    "        num_samples_pressed = recording[press_inds,:].shape[0]\n",
    "        \n",
    "        # CASE 0: check that we have at least 300 samples in entire recording\n",
    "        if recording.shape[0] < NUM_SAMPLES_PER_CHUNK:\n",
    "            sample_pad = np.zeros((NUM_SAMPLES_PER_CHUNK,NUM_MEASUREMENTS_PER_SAMPLE))\n",
    "            sample_pad[0:recording.shape[0],:] = recording[:,1:]\n",
    "            out.append(sample_pad)\n",
    "        \n",
    "        # CASE 1: we have enough  samples to fill out chunks with only press data\n",
    "        elif num_samples_pressed >= NUM_SAMPLES_PER_CHUNK + NUM_CHUNKS_PER_RECORDING:\n",
    "            \n",
    "            # First chunk (always capture very beginning of data)\n",
    "            first_chunk_start = press_inds[0]\n",
    "            out.append(recording[first_chunk_start:first_chunk_start+NUM_SAMPLES_PER_CHUNK,1:])\n",
    "            \n",
    "            # Last chunk (always capture very end of data)\n",
    "            last_chunk_start = press_inds[-1]-NUM_SAMPLES_PER_CHUNK+1\n",
    "            out.append(recording[last_chunk_start:last_chunk_start+NUM_SAMPLES_PER_CHUNK,1:])\n",
    "            \n",
    "            # Middle chunks (evenly space from first chunk for simplicity)\n",
    "            step = num_samples_pressed // NUM_CHUNKS_PER_RECORDING\n",
    "            prev_chunk_start = first_chunk_start\n",
    "            for i in range(NUM_CHUNKS_PER_RECORDING-2):\n",
    "                next_chunk_start = prev_chunk_start+step\n",
    "                out.append(recording[next_chunk_start:next_chunk_start+NUM_SAMPLES_PER_CHUNK,1:])\n",
    "                prev_chunk_start = next_chunk_start\n",
    "                \n",
    "        # CASE 2: We don't have enough samples to fill chunks: need to pull from surrounding data\n",
    "        elif num_samples_pressed < NUM_SAMPLES_PER_CHUNK + NUM_CHUNKS_PER_RECORDING:\n",
    "            # First chunk: captures until end of recording, starting however many samples before needed to do that\n",
    "            first_chunk_start = max(0, press_inds[-1]-NUM_SAMPLES_PER_CHUNK+1)\n",
    "            out.append(recording[first_chunk_start:first_chunk_start+NUM_SAMPLES_PER_CHUNK,1:])\n",
    "            \n",
    "            # Last chunk: captures from start of recording, going however many samples after needed to do that\n",
    "            last_chunk_end = min(recording.shape[0], press_inds[0]+NUM_SAMPLES_PER_CHUNK)\n",
    "            out.append(recording[last_chunk_end-NUM_SAMPLES_PER_CHUNK:last_chunk_end,1:])\n",
    "            \n",
    "            # Middle chunks: (evenly space from first chunk for simplicity)\n",
    "            step = (last_chunk_end - first_chunk_start+1) // NUM_CHUNKS_PER_RECORDING\n",
    "            prev_chunk_start = first_chunk_start\n",
    "            for i in range(NUM_CHUNKS_PER_RECORDING-2):\n",
    "                next_chunk_start = prev_chunk_start+step\n",
    "                out.append(recording[next_chunk_start:next_chunk_start+NUM_SAMPLES_PER_CHUNK,1:])\n",
    "                prev_chunk_start = next_chunk_start\n",
    "                \n",
    "    return out\n",
    "    \n",
    "# Note: train obtained from running Splitting_up_data.ipynb on raw training data\n",
    "train = ['Data/A_lauren_11.csv', 'Data/A_lauren_18.csv', 'Data/A_lauren_20.csv', 'Data/B_lauren_14.csv',\\\n",
    "         'Data/B_lauren_15.csv', 'Data/B_lauren_17.csv', 'Data/C_lauren_28.csv', 'Data/C_lauren_18.csv', \\\n",
    "         'Data/C_lauren_3.csv', 'Data/D_lauren_8.csv', 'Data/D_lauren_23.csv', 'Data/D_lauren_4.csv',\\\n",
    "         'Data/E_lauren_5.csv', 'Data/E_lauren_7.csv', 'Data/E_lauren_25.csv', 'Data/F_lauren_4.csv', \\\n",
    "         'Data/F_lauren_26.csv', 'Data/F_lauren_2.csv', 'Data/G_lauren_4.csv', 'Data/G_lauren_8.csv',\\\n",
    "         'Data/G_lauren_13.csv', 'Data/H_lauren_29.csv', 'Data/H_lauren_23.csv', 'Data/H_lauren_22.csv', \\\n",
    "         'Data/I_lauren_19.csv', 'Data/I_lauren_22.csv', 'Data/I_lauren_8.csv', 'Data/J_lauren_1.csv', \\\n",
    "         'Data/J_lauren_16.csv', 'Data/J_lauren_23.csv', 'Data/K_lauren_1.csv', 'Data/K_lauren_10.csv', \\\n",
    "         'Data/K_lauren_23.csv', 'Data/L_lauren_30.csv', 'Data/L_lauren_6.csv', 'Data/L_lauren_26.csv', \\\n",
    "         'Data/M_lauren_21.csv', 'Data/M_lauren_3.csv', 'Data/M_lauren_19.csv', 'Data/N_lauren_1.csv', \\\n",
    "         'Data/N_lauren_14.csv', 'Data/N_lauren_25.csv', 'Data/O_lauren_17.csv', 'Data/O_lauren_10.csv', \\\n",
    "         'Data/O_lauren_28.csv', 'Data/P_lauren_22.csv', 'Data/P_lauren_29.csv', 'Data/P_lauren_3.csv', \\\n",
    "         'Data/Q_lauren_10.csv', 'Data/Q_lauren_29.csv', 'Data/Q_lauren_23.csv', 'Data/R_lauren_4.csv', \\\n",
    "         'Data/R_lauren_8.csv', 'Data/R_lauren_12.csv', 'Data/S_lauren_27.csv', 'Data/S_lauren_12.csv', \\\n",
    "         'Data/S_lauren_18.csv', 'Data/T_lauren_9.csv', 'Data/T_lauren_21.csv', 'Data/T_lauren_7.csv', \\\n",
    "         'Data/U_lauren_11.csv', 'Data/U_lauren_24.csv', 'Data/U_lauren_3.csv', 'Data/V_lauren_5.csv', \\\n",
    "         'Data/V_lauren_26.csv', 'Data/V_lauren_10.csv', 'Data/W_lauren_19.csv', 'Data/W_lauren_2.csv', \\\n",
    "         'Data/W_lauren_18.csv', 'Data/X_lauren_2.csv', 'Data/X_lauren_28.csv', 'Data/X_lauren_8.csv', \\\n",
    "         'Data/Y_lauren_29.csv', 'Data/Y_lauren_3.csv', 'Data/Y_lauren_17.csv', 'Data/Z_lauren_8.csv', \\\n",
    "         'Data/Z_lauren_20.csv', 'Data/Z_lauren_23.csv']\n",
    "recordings = csv_to_np(train)\n",
    "assert len(recordings) == len(train)\n",
    "NUM_SAMPLES_PER_CHUNK = 300\n",
    "NUM_MEASUREMENTS_PER_SAMPLE = 3\n",
    "NUM_CHUNKS_PER_RECORDING = 5\n",
    "chunks = chunk_data(recordings, NUM_SAMPLES_PER_CHUNK, NUM_MEASUREMENTS_PER_SAMPLE, NUM_CHUNKS_PER_RECORDING)\n",
    "print(\"executed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success!\n"
     ]
    }
   ],
   "source": [
    "def test_chunk_data_size(chunks):\n",
    "    # confirm samples are of length NUM_SAMPLES_PER_CHUNK\n",
    "    for chunk in chunks:\n",
    "        assert chunk.shape == (NUM_SAMPLES_PER_CHUNK,NUM_MEASUREMENTS_PER_SAMPLE)\n",
    "        \n",
    "\n",
    "def test_chunk_toy_data():\n",
    "    ##################### TEST #####################\n",
    "    # description: test perfect coverage of chunk \n",
    "    data = [np.array([[0, 1], \n",
    "                     [0, 2],\n",
    "                     [1, 3],\n",
    "                     [1, 4],\n",
    "                     [0, 5],\n",
    "                     [0, 6]])]\n",
    "    # order: samples per chunk, meas per sample, chunks per recording\n",
    "    out = chunk_data(data, 2, 1, 1)\n",
    "    assert np.all(out == np.array([[3],[4]]))\n",
    "    \n",
    "    ##################### TEST #####################\n",
    "    # description: test perfect coverage of chunk, 2 recordings\n",
    "    data = [np.array([[0, 1], \n",
    "                     [0, 2],\n",
    "                     [1, 3],\n",
    "                     [1, 4],\n",
    "                     [0, 5],\n",
    "                     [0, 6]]), \n",
    "            np.array([[0, 1], \n",
    "                     [0, 2],\n",
    "                     [1, 3],\n",
    "                     [1, 4],\n",
    "                     [0, 5],\n",
    "                     [0, 6]])]\n",
    "    # order: samples per chunk, meas per sample, chunks per recording\n",
    "    out = chunk_data(data, 2, 1, 1)\n",
    "    assert np.all(out[0] == np.array([[3],[4]]))\n",
    "    assert np.all(out[1] == np.array([[3],[4]]))\n",
    "    \n",
    "    ##################### TEST #####################\n",
    "    # description: test grabbing prev/following values if button press inds < chunk size\n",
    "    data = [np.array([[0, 1], \n",
    "                     [0, 2],\n",
    "                     [1, 3],\n",
    "                     [1, 4],\n",
    "                     [0, 5],\n",
    "                     [0, 6]])]\n",
    "    out = chunk_data(data, 3, 1, 2)\n",
    "    assert np.all(out[0] == np.array([[2],[3],[4]]))\n",
    "    assert np.all(out[1] ==  np.array([[3],[4],[5]]))\n",
    "    \n",
    "    ##################### TEST #####################\n",
    "    # description: test grabbing prev with potential out of bounds error at beginning with 1 chunk\n",
    "    data = [np.array([[1, 1], \n",
    "                     [1, 2],\n",
    "                     [1, 3],\n",
    "                     [0, 4],\n",
    "                     [0, 5],\n",
    "                     [0, 6]])]\n",
    "    out = chunk_data(data, 4, 1, 1)\n",
    "    assert np.all(out[0] == np.array([[1],[2],[3],[4]]))\n",
    "    \n",
    "    ##################### TEST ####################\n",
    "    # description: test grabbing prev with potential out of bounds error at beginning with 2 chunks\n",
    "    data = [np.array([[1, 1], \n",
    "                     [1, 2],\n",
    "                     [1, 3],\n",
    "                     [0, 4],\n",
    "                     [0, 5],\n",
    "                     [0, 6]])]\n",
    "    out = chunk_data(data, 4, 1, 2)\n",
    "    assert np.all(out[0] == np.array([[1],[2],[3],[4]]))\n",
    "    assert np.all(out[1] == np.array([[1],[2],[3],[4]]))\n",
    "    \n",
    "    \n",
    "    ##################### TEST #####################\n",
    "    # description: test grabbing prev with potential out of bounds error at end with 1 chunk\n",
    "    data = [np.array([[0, 1], \n",
    "                     [0, 2],\n",
    "                     [0, 3],\n",
    "                     [0, 4],\n",
    "                     [1, 5],\n",
    "                     [1, 6]])]\n",
    "    out = chunk_data(data, 3, 1, 1)\n",
    "    assert np.all(out[0] == np.array([[4],[5],[6]]))\n",
    "    \n",
    "     ##################### TEST #####################\n",
    "    # description: test grabbing prev with potential out of bounds error at end with 2 chunks\n",
    "    data = [np.array([[0, 1], \n",
    "                     [0, 2],\n",
    "                     [0, 3],\n",
    "                     [0, 4],\n",
    "                     [1, 5],\n",
    "                     [1, 6]])]\n",
    "    out = chunk_data(data, 3, 1, 2)\n",
    "    assert np.all(out[0] == np.array([[4],[5],[6]]))\n",
    "    assert np.all(out[1] == np.array([[4],[5],[6]]))\n",
    "    \n",
    "    ##################### TEST #####################\n",
    "    # description: test full sample in recording with uneven split\n",
    "    data = [np.array([[1, 1], \n",
    "                     [1, 2],\n",
    "                     [1, 3],\n",
    "                     [1, 4],\n",
    "                     [1, 5],\n",
    "                     [1, 6],\n",
    "                     [1, 7]])]\n",
    "    \n",
    "    # num_samples_pressed >= NUM_SAMPLES_PER_CHUNK + NUM_CHUNKS_PER_RECORDING ? 7 > 2 + 3 = 5 yes\n",
    "    # step: num_samples_pressed // NUM_CHUNKS_PER_RECORDING = 7 // 3 = 2\n",
    "    out = chunk_data(data, 2, 1, 3) # step: (7-3)//2 = 2\n",
    "    assert np.all(out[0] == np.array([[1],[2]])) # first\n",
    "    assert np.all(out[1] == np.array([[6],[7]])) # middle\n",
    "    assert np.all(out[2] ==  np.array([[3],[4]])) # last\n",
    "    \n",
    "    \n",
    "    ##################### TEST #####################\n",
    "    # description: test chunk size that is greater than recording size\n",
    "    data = [np.array([[0, 1], \n",
    "                     [0, 2],\n",
    "                     [1, 3],\n",
    "                     [1, 4],\n",
    "                     [0, 5],\n",
    "                     [0, 6]])]\n",
    "    out = chunk_data(data, 8, 1, 1)\n",
    "    assert np.all(out == np.array([[1],[2],[3],[4],[5],[6],[0],[0]]))\n",
    "    \n",
    "    \n",
    "    \n",
    "test_chunk_data_size(chunks)\n",
    "test_chunk_toy_data()\n",
    "print(\"success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
